#!/bin/env python3

import logging
import argparse
import pandas as pd
import re
import wordcloud
import nltk
import numpy as np
import os

from collections import Counter
from nltk.tokenize import word_tokenize
from PIL import Image

FORMAT = '%(asctime)s %(processName)s[%(process)d] %(levelname)s %(module)s %(name)s %(message)s'
logging.basicConfig(format=FORMAT)
logger = logging.getLogger(__name__)
# nltk.download('stopwords')
# nltk.download('punkt')

STOPWORDS = {
    'empresa': nltk.corpus.stopwords.words("portuguese") + ['ltda', 'sa', 's.a', 's.a.', 'ltd', 'paulo'],
    'servico': nltk.corpus.stopwords.words("portuguese") + ['outros', 'etc'],
    'estado': [],
    'area': nltk.corpus.stopwords.words("portuguese") + ['outros', 'etc'],
}

MAXWORDS = int(os.environ.get('MAXWORDS', '40'))
MINFONTSIZE = int(os.environ.get('MINFONTSIZE', '10'))
CONTOURWIDTH = int(os.environ.get('CONTOURWIDTH', '6'))
CONTOURCOLOR = os.environ.get('CONTOURCOLOR', 'orange')
BACKGROUNDCOLOR = os.environ.get('BACKGROUNDCOLOR', 'black')
COLORMAP = os.environ.get('COLORMAP', 'Oranges_r')

MAPPINGS = {
    'empresa': {
        's.a': 'sa',
    },
    'estado': {
        'ac': 'acre',
        'al': 'alagoas',
        'ap': 'amapá',
        'am': 'amazonas',
        'ba': 'bahia',
        'ce': 'ceará',
        'df': 'distrito federal',
        'es': 'espírito santo',
        'go': 'goiás',
        'ma': 'maranhão',
        'mt': 'mato grosso',
        'ms': 'mato grosso do sul',
        'mg': 'minas gerais',
        'pa': 'pará',
        'pb': 'paraíba',
        'pr': 'curitiba',
        'pe': 'pernambuco',
        'pi': 'piauí',
        'rj': 'rio de janeiro',
        'rn': 'rio grande do norte',
        'rs': 'rio grande do sul',
        'ro': 'rondônia',
        'rr': 'roraima',
        'sc': 'santa catarina',
        'sp': 'são paulo',
        'se': 'sergipe',
        'to': 'tocantins'
    },
}


def extrai_tokens(df, raw_mode=False, stopwords=[], mapping={}):
    # extrai os valores nulos
    df.dropna(inplace=True)

    df['tokens'] = df[df.columns.values[0]]

    if raw_mode:
        # remove o que houver entre parenteses
        logger.info('Removendo conteúdo entre parênteses...')
        rule_entre_parenteses = lambda x: re.sub(r"""\(.+\)""", '', x)
        df['tokens'] = df['tokens'].map(rule_entre_parenteses)
        logger.info('Removendo conteúdo entre parênteses...ok')

    # trata caracteres especiais
    logger.info('Removendo caracteres especiais...')
    transformacoes = {' ': ' ', '/': '', ',': '', '\(': '', '\)': '', "-": '', r"^\t": '', '\.$': '', ',': ''}
    df['tokens'] = df['tokens'].replace(transformacoes, regex=True)
    logger.info('Removendo caracteres especiais...ok')

    # converte tudo para lower case
    logger.info('Convertendo caracteres para caixa baixa...')
    df['tokens'] = df['tokens'].map(str.lower)
    logger.info('Convertendo caracteres para caixa baixa...ok')

    # tokeniza cada um dos registros e retira as stopwords
    logger.info('Tokenizando os registros e removendo as stopwords...')
    if raw_mode:
        df['tokens'] = df['tokens'].map(
            lambda x: [token for token in word_tokenize(x, language='portuguese') if token not in stopwords]
        )
    else:
        df['tokens'] = df['tokens'].map(
            lambda x: [token for token in word_tokenize(x, language='portuguese')]
        )
    logger.info('Tokenizando os registros e removendo as stopwords...ok')

    # faz remapeamento de tokens
    if mapping:
        logger.info('Aplicando remapeamento de tokens...')
        df['tokens'] = df['tokens'].map(lambda t: [mapping.get(token, token) for token in t])
        logger.info('Aplicando remapeamento de tokens...ok')

    # removendo tokens com comprimento == 1
    logger.info('Removendo tokens com comprimento == 1...')
    df['tokens'] = df['tokens'].map(lambda x: [token for token in x if len(token) > 1])
    logger.info('Removendo tokens com comprimento == 1...ok')

    if raw_mode:
        return [token for tokens_list in df['tokens'] for token in tokens_list]
    else:
        return [' '.join(tokens_list) for tokens_list in df['tokens']]


if __name__ == "__main__":

    parser = argparse.ArgumentParser(description='Análise de reclamações de empresas por WordCloud')
    parser.add_argument("--target", type=str, choices=('empresa', 'servico', 'estado', 'area'), default='empresa',
                        help="Atributo de análise. (Default: 'empresa')")
    parser.add_argument("-r", "--raw-mode", action='store_true', default=False,
                        help="Processa os registros mas não tokeniza. (Default: False)")
    parser.add_argument("-v", "--verbose", action='store_true', default=False,
                        help="Executa em modo verbose. (Default: False)")
    parser.add_argument("-o", "--output-file", type=argparse.FileType('wb', 0), required=False, default=None,
                        help="Arquivo de saída. (Default: 'wordcloud_[TARGET].png')")
    parser.add_argument("-m", "--mask-file", type=argparse.FileType('rb', 0), required=False, default=None,
                        help=f"Arquivo de imagem para utilizar como máscara. (Default: 'img/brazil_mask.png' \
                        (mais de {MAXWORDS} tokens) ou 'img/full_mask.png' (menos de {MAXWORDS} tokens))")

    args = parser.parse_args()

    if not args.verbose:
        logger.setLevel(logging.WARNING)
    else:
        logger.setLevel(logging.INFO)

    logger.info('Iniciando...')

    # carrega a base de dados no data frame
    df = pd.read_csv('db/base_reclamacoes.csv', encoding='windows-1252', sep=';')

    # ajusta nomes das colunas
    df['area'] = df['area ']
    df['servico'] = df['serviço']
    df.drop(columns=['area ', 'serviço'])

    # extrai os tokens da coluna
    # informada em args.target (empresa, serviço, estado)
    tokens = extrai_tokens(
        pd.DataFrame(data=df[args.target]),
        mapping=MAPPINGS.get(args.target, {}),
        stopwords=STOPWORDS.get(args.target, []),
        raw_mode=not args.raw_mode
    )

    logger.debug(f'Tokens extraidos: {tokens[:10]}')

    tokens_frequencies = Counter(tokens)

    if not args.mask_file:
        if len(tokens_frequencies) >= MAXWORDS:
            mask = np.array(Image.open('img/brazil_mask.png'))
        else:
            mask = np.array(Image.open('img/full_mask.png'))
    else:
        mask = np.array(Image.open(args.mask_file))

    wordcloud_image = wordcloud.WordCloud(
        background_color=BACKGROUNDCOLOR,
        colormap=COLORMAP,
        mask=mask,
        width=mask.shape[1],
        height=mask.shape[0],
        min_font_size=MINFONTSIZE,
        contour_width=CONTOURWIDTH,
        contour_color=CONTOURCOLOR,
        max_words=MAXWORDS
    ).fit_words(tokens_frequencies)

    if args.output_file:
        wordcloud_image.to_image().save(args.output_file)
    else:
        wordcloud_image.to_image().save(f'wordcloud_{args.target}.png', 'png')

    logger.info('... fim')
